{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051cc9e9-735f-40ec-9c73-7ad948ebbcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antal tecken i text: 290\n",
      "\n",
      "du är en liten språkmodell som tränas på text.\n",
      "du kan lära dig mönster i sekvenser av tecken.\n",
      "det här är bara exempeltext, men den är lite längre än tidigare.\n",
      "vi kan lägga till fler meningar för att \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import textwrap\n",
    "\n",
    "# 1. \"Bättre\" leksaksdata – längre text\n",
    "raw_text = \"\"\"\n",
    "Du är en liten språkmodell som tränas på text.\n",
    "Du kan lära dig mönster i sekvenser av tecken.\n",
    "Det här är bara exempeltext, men den är lite längre än tidigare.\n",
    "Vi kan lägga till fler meningar för att göra träningen mer intressant.\n",
    "Språkmodellen ska försöka förutsäga nästa tecken i texten.\n",
    "\"\"\".lower()\n",
    "\n",
    "# Ta bort leading indentation\n",
    "text = textwrap.dedent(raw_text)\n",
    "\n",
    "print(\"Antal tecken i text:\", len(text))\n",
    "print(text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0daddd8-20e3-46ac-b020-68dcb8ecdcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antal unika tecken: 28\n",
      "['\\n', ' ', ',', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'x', 'ä', 'å', 'ö']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"Antal unika tecken:\", vocab_size)\n",
    "print(chars)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "\n",
    "def encode(s: str):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(ids):\n",
    "    return \"\".join(itos[i] for i in ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ddcaf7f-866f-4f8d-bf25-cbe4b1961b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total längd på data: 290\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "block_size = 64   # hur långa sekvenser modellen får se\n",
    "print(\"Total längd på data:\", len(data))\n",
    "\n",
    "def get_batch(batch_size=32, split=\"train\"):\n",
    "    # För enkelhet: inga valideringssplit nu, bara slumpbatchar\n",
    "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27233e73-61a1-4108-991a-7d8e64d4f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, block_size):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        # Causal mask (T, T)\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # 1) Linjära projektioner\n",
    "        Q = self.W_q(x)  # (B, T, C)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # 2) Dela upp i heads: (B, T, H, d_head) -> (B, H, T, d_head)\n",
    "        Q = Q.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # 3) Scaled dot-product attention\n",
    "        # scores: (B, H, T, T)\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.d_head)\n",
    "\n",
    "        # 4) Causal mask\n",
    "        mask = self.mask[:T, :T]  # (T, T)\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # 5) Softmax över \"vem tittar jag på\"\n",
    "        A = F.softmax(scores, dim=-1)  # (B, H, T, T)\n",
    "\n",
    "        # 6) Vägda summor\n",
    "        out = A @ V  # (B, H, T, d_head)\n",
    "\n",
    "        # 7) Tillbaka till (B, T, C)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # 8) Ut-projektion\n",
    "        out = self.W_o(out)  # (B, T, C)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e1ddfb0-3fef-437a-8d98-672b82c1a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, block_size):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(d_model, num_heads, block_size)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention del\n",
    "        x = x + self.attn(self.ln1(x))  # pre-norm variant\n",
    "        # FFN-del\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c49418b1-4717-49d9-9e9a-d1beb429b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPTChar(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, d_model=128, num_heads=4, d_ff=256, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, block_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: (B, T)\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "\n",
    "        pos = torch.arange(T, device=idx.device)\n",
    "        tok_emb = self.token_emb(idx)          # (B, T, d_model)\n",
    "        pos_emb = self.pos_emb(pos)[None, :, :]  # (1, T, d_model)\n",
    "        x = tok_emb + pos_emb                  # (B, T, d_model)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)                  # (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits_flat = logits.view(B*T, C)\n",
    "            targets_flat = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=100):\n",
    "        # idx: (B, T_start)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits_last = logits[:, -1, :]  # (B, vocab_size)\n",
    "            probs = F.softmax(logits_last, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b235abc-faad-4c00-885b-3fd926e8cd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Steg 0, loss 3.6161\n",
      "Steg 200, loss 0.7941\n",
      "Steg 400, loss 0.1129\n",
      "Steg 600, loss 0.0623\n",
      "Steg 800, loss 0.0587\n",
      "Steg 1000, loss 0.0491\n",
      "Steg 1200, loss 0.0466\n",
      "Steg 1400, loss 0.0415\n",
      "Steg 1600, loss 0.0453\n",
      "Steg 1800, loss 0.0445\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model = MiniGPTChar(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    d_ff=256,\n",
    "    num_layers=3,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for step in range(2000):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(batch_size=32)\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Steg {step}, loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92bca34a-b0e8-4a69-a758-a3647f28c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "språkmodell som tränas på text.\n",
      "du kan lära dig mönster i sekvenser av tecken.\n",
      "det här är bara exempeltext, men den är lite längre än tidigare.\n",
      "vi kan lägga till fler meningar för att göra träningen mer in\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "start_text = \"språk\"\n",
    "start_ids = torch.tensor([encode(start_text)], dtype=torch.long).to(device)\n",
    "\n",
    "gen_ids = model.generate(start_ids, max_new_tokens=200)\n",
    "print(decode(gen_ids[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32496d6-3291-4060-b5ee-f4cdf0d815de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
